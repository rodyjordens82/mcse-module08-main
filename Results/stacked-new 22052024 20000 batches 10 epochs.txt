(base) PS C:\Users\rodyj\Documents\Module8>  c:; cd 'c:\Users\rodyj\Documents\Module8'; & 'c:\Users\rodyj\miniconda3\envs\python3.11\python.exe' 'c:\Users\rodyj\.vscode\extensions\ms-python.debugpy-2024.6.0-win32-x64\bundled\libs\debugpy\adapter/../..\debugpy\launcher' '52011' '--' 'C:\Users\rodyj\Documents\Module8\new_stacked.py'
0.00100  Loading Dataset
39.67450         Cleaning Dataset
41.75568         Encoding Dataset
56.68401         Encoding labels
59.29196         Saving encoded dataset to .\encoded_dataset.pkl
60.86745         Splitting Test and Train Data
66.82588         Performing StandardScaler
71.13469         Converting to Tensors
75.89640         Training models
75.89738         Training LSTM
101.25922        [1,  2000/20034] loss: 0.009
118.97109        [1,  4000/20034] loss: 0.005
136.25868        [1,  6000/20034] loss: 0.005
153.48501        [1,  8000/20034] loss: 0.005
170.65676        [1, 10000/20034] loss: 0.005
187.82986        [1, 12000/20034] loss: 0.005
205.16070        [1, 14000/20034] loss: 0.005
222.40414        [1, 16000/20034] loss: 0.004
239.61258        [1, 18000/20034] loss: 0.004
256.86500        [1, 20000/20034] loss: 0.004
258.85586        Epoch [1/10], Loss: 0.00026
283.38845        [2,  2000/20034] loss: 0.004
300.54790        [2,  4000/20034] loss: 0.003
317.79382        [2,  6000/20034] loss: 0.003
334.98623        [2,  8000/20034] loss: 0.003
352.27590        [2, 10000/20034] loss: 0.004
370.09388        [2, 12000/20034] loss: 0.004
388.76723        [2, 14000/20034] loss: 0.003
412.09522        [2, 16000/20034] loss: 0.004
430.00149        [2, 18000/20034] loss: 0.003
447.23236        [2, 20000/20034] loss: 0.003
449.18776        Epoch [2/10], Loss: 0.00029
474.59967        [3,  2000/20034] loss: 0.004
491.92157        [3,  4000/20034] loss: 0.003
509.60461        [3,  6000/20034] loss: 0.003
527.58268        [3,  8000/20034] loss: 0.003
544.76670        [3, 10000/20034] loss: 0.003
562.01660        [3, 12000/20034] loss: 0.003
579.26364        [3, 14000/20034] loss: 0.003
597.05673        [3, 16000/20034] loss: 0.003
614.66491        [3, 18000/20034] loss: 0.003
632.02766        [3, 20000/20034] loss: 0.003
633.84563        Epoch [3/10], Loss: 0.00318
658.18611        [4,  2000/20034] loss: 0.003
675.35296        [4,  4000/20034] loss: 0.003
692.50620        [4,  6000/20034] loss: 0.003
709.70496        [4,  8000/20034] loss: 0.003
726.83100        [4, 10000/20034] loss: 0.003
743.91187        [4, 12000/20034] loss: 0.003
761.09620        [4, 14000/20034] loss: 0.003
778.29082        [4, 16000/20034] loss: 0.003
795.38512        [4, 18000/20034] loss: 0.003
812.52078        [4, 20000/20034] loss: 0.003
814.38476        Epoch [4/10], Loss: 0.00000
838.86252        [5,  2000/20034] loss: 0.003
856.21422        [5,  4000/20034] loss: 0.003
873.53142        [5,  6000/20034] loss: 0.003
890.67003        [5,  8000/20034] loss: 0.003
907.73739        [5, 10000/20034] loss: 0.003
924.82403        [5, 12000/20034] loss: 0.003
941.90140        [5, 14000/20034] loss: 0.003
958.99233        [5, 16000/20034] loss: 0.003
976.03380        [5, 18000/20034] loss: 0.003
993.09285        [5, 20000/20034] loss: 0.003
994.91499        Epoch [5/10], Loss: 0.00039
1019.43193       [6,  2000/20034] loss: 0.003
1036.73606       [6,  4000/20034] loss: 0.003
1053.97422       [6,  6000/20034] loss: 0.003
1071.20516       [6,  8000/20034] loss: 0.003
1088.49594       [6, 10000/20034] loss: 0.003
1105.98665       [6, 12000/20034] loss: 0.003
1123.47715       [6, 14000/20034] loss: 0.003
1141.03773       [6, 16000/20034] loss: 0.003
1158.69734       [6, 18000/20034] loss: 0.003
1176.35314       [6, 20000/20034] loss: 0.003
1178.30502       Epoch [6/10], Loss: 0.00015
1203.05099       [7,  2000/20034] loss: 0.003
1220.07284       [7,  4000/20034] loss: 0.003
1237.30476       [7,  6000/20034] loss: 0.003
1254.40257       [7,  8000/20034] loss: 0.003
1271.47366       [7, 10000/20034] loss: 0.003
1288.56009       [7, 12000/20034] loss: 0.003
1305.63470       [7, 14000/20034] loss: 0.003
1322.73472       [7, 16000/20034] loss: 0.003
1339.96233       [7, 18000/20034] loss: 0.003
1357.01060       [7, 20000/20034] loss: 0.003
1358.86175       Epoch [7/10], Loss: 0.00004
1383.38647       [8,  2000/20034] loss: 0.003
1400.61336       [8,  4000/20034] loss: 0.003
1417.86342       [8,  6000/20034] loss: 0.003
1435.11847       [8,  8000/20034] loss: 0.003
1452.25923       [8, 10000/20034] loss: 0.003
1469.53328       [8, 12000/20034] loss: 0.003
1486.81475       [8, 14000/20034] loss: 0.003
1504.09287       [8, 16000/20034] loss: 0.003
1521.29277       [8, 18000/20034] loss: 0.003
1538.52889       [8, 20000/20034] loss: 0.003
1540.50646       Epoch [8/10], Loss: 0.00059
1564.95055       [9,  2000/20034] loss: 0.003
1582.07946       [9,  4000/20034] loss: 0.003
1599.19807       [9,  6000/20034] loss: 0.003
1616.52341       [9,  8000/20034] loss: 0.003
1634.22764       [9, 10000/20034] loss: 0.002
1651.48330       [9, 12000/20034] loss: 0.003
1668.69494       [9, 14000/20034] loss: 0.003
1685.86220       [9, 16000/20034] loss: 0.002
1703.13632       [9, 18000/20034] loss: 0.003
1720.46982       [9, 20000/20034] loss: 0.002
1722.43547       Epoch [9/10], Loss: 0.00009
1746.87335       [10,  2000/20034] loss: 0.003
1763.97718       [10,  4000/20034] loss: 0.002
1781.07917       [10,  6000/20034] loss: 0.002
1798.19322       [10,  8000/20034] loss: 0.003
1815.26847       [10, 10000/20034] loss: 0.002
1832.36602       [10, 12000/20034] loss: 0.003
1849.44090       [10, 14000/20034] loss: 0.003
1866.53648       [10, 16000/20034] loss: 0.003
1883.70018       [10, 18000/20034] loss: 0.003
1900.85913       [10, 20000/20034] loss: 0.003
1902.82614       Epoch [10/10], Loss: 0.00000
2071.59130       Training ANN
2094.02695       [1,  2000/20034] loss: 0.011
2109.78598       [1,  4000/20034] loss: 0.008
2125.67862       [1,  6000/20034] loss: 0.009
2141.44620       [1,  8000/20034] loss: 0.007
2157.22111       [1, 10000/20034] loss: 0.014
2173.02326       [1, 12000/20034] loss: 0.017
2188.87538       [1, 14000/20034] loss: 0.011
2204.65770       [1, 16000/20034] loss: 0.006
2220.56682       [1, 18000/20034] loss: 0.006
2236.43938       [1, 20000/20034] loss: 0.005
2237.90068       Epoch [1/10], Loss: 0.00179
2260.53522       [2,  2000/20034] loss: 0.009
2276.31250       [2,  4000/20034] loss: 0.006
2292.07030       [2,  6000/20034] loss: 0.011
2307.82921       [2,  8000/20034] loss: 0.006
2323.58001       [2, 10000/20034] loss: 0.030
2339.27306       [2, 12000/20034] loss: 0.013
2355.06106       [2, 14000/20034] loss: 0.006
2370.78279       [2, 16000/20034] loss: 0.005
2386.52098       [2, 18000/20034] loss: 0.005
2402.24601       [2, 20000/20034] loss: 0.004
2403.56969       Epoch [2/10], Loss: 0.00126
2426.32048       [3,  2000/20034] loss: 0.009
2442.28579       [3,  4000/20034] loss: 0.006
2458.15758       [3,  6000/20034] loss: 0.009
2474.06101       [3,  8000/20034] loss: 0.007
2489.92166       [3, 10000/20034] loss: 0.025
2505.84570       [3, 12000/20034] loss: 0.012
2521.76960       [3, 14000/20034] loss: 0.004
2537.65833       [3, 16000/20034] loss: 0.004
2553.48492       [3, 18000/20034] loss: 0.004
2569.40151       [3, 20000/20034] loss: 0.004
2570.83295       Epoch [3/10], Loss: 0.00111
2593.80517       [4,  2000/20034] loss: 0.006
2609.57697       [4,  4000/20034] loss: 0.004
2625.44067       [4,  6000/20034] loss: 0.004
2641.23972       [4,  8000/20034] loss: 0.005
2657.02245       [4, 10000/20034] loss: 0.020
2672.80871       [4, 12000/20034] loss: 0.017
2688.64130       [4, 14000/20034] loss: 0.007
2704.43603       [4, 16000/20034] loss: 0.028
2720.19700       [4, 18000/20034] loss: 0.004
2736.00890       [4, 20000/20034] loss: 0.004
2737.41605       Epoch [4/10], Loss: 0.00087
2759.93023       [5,  2000/20034] loss: 0.008
2775.66590       [5,  4000/20034] loss: 0.004
2791.38233       [5,  6000/20034] loss: 0.006
2807.08790       [5,  8000/20034] loss: 0.004
2822.80215       [5, 10000/20034] loss: 0.022
2838.57300       [5, 12000/20034] loss: 0.021
2854.36750       [5, 14000/20034] loss: 0.004
2870.08430       [5, 16000/20034] loss: 0.004
2885.78649       [5, 18000/20034] loss: 0.004
2901.46662       [5, 20000/20034] loss: 0.003
2902.94022       Epoch [5/10], Loss: 0.00070
2925.56934       [6,  2000/20034] loss: 0.006
2941.31600       [6,  4000/20034] loss: 0.004
2957.11645       [6,  6000/20034] loss: 0.004
2972.92147       [6,  8000/20034] loss: 0.007
2988.72510       [6, 10000/20034] loss: 0.016
3004.56055       [6, 12000/20034] loss: 0.016
3020.44637       [6, 14000/20034] loss: 0.004
3036.26704       [6, 16000/20034] loss: 0.004
3052.05484       [6, 18000/20034] loss: 0.003
3067.87253       [6, 20000/20034] loss: 0.003
3069.36371       Epoch [6/10], Loss: 0.00099
3091.80827       [7,  2000/20034] loss: 0.008
3107.41579       [7,  4000/20034] loss: 0.004
3123.05575       [7,  6000/20034] loss: 0.004
3138.70252       [7,  8000/20034] loss: 0.006
3154.38018       [7, 10000/20034] loss: 0.029
3170.01711       [7, 12000/20034] loss: 0.008
3185.72285       [7, 14000/20034] loss: 0.007
3201.33119       [7, 16000/20034] loss: 0.018
3217.01132       [7, 18000/20034] loss: 0.003
3232.69642       [7, 20000/20034] loss: 0.004
3233.97923       Epoch [7/10], Loss: 0.00071
3256.53870       [8,  2000/20034] loss: 0.006
3272.38643       [8,  4000/20034] loss: 0.003
3288.26344       [8,  6000/20034] loss: 0.003
3304.24131       [8,  8000/20034] loss: 0.004
3320.01443       [8, 10000/20034] loss: 0.009
3335.80633       [8, 12000/20034] loss: 0.013
3351.70798       [8, 14000/20034] loss: 0.008
3367.49463       [8, 16000/20034] loss: 0.003
3383.25471       [8, 18000/20034] loss: 0.003
3399.12562       [8, 20000/20034] loss: 0.003
3400.45780       Epoch [8/10], Loss: 0.00104
3423.01370       [9,  2000/20034] loss: 0.004
3438.84716       [9,  4000/20034] loss: 0.003
3454.56970       [9,  6000/20034] loss: 0.003
3470.32133       [9,  8000/20034] loss: 0.005
3486.05054       [9, 10000/20034] loss: 0.018
3501.76844       [9, 12000/20034] loss: 0.013
3517.56525       [9, 14000/20034] loss: 0.003
3533.32215       [9, 16000/20034] loss: 0.003
3549.00976       [9, 18000/20034] loss: 0.003
3564.69906       [9, 20000/20034] loss: 0.004
3566.21317       Epoch [9/10], Loss: 0.00070
3588.97837       [10,  2000/20034] loss: 0.004
3604.89937       [10,  4000/20034] loss: 0.004
3620.83588       [10,  6000/20034] loss: 0.003
3636.69844       [10,  8000/20034] loss: 0.005
3652.61598       [10, 10000/20034] loss: 0.035
3668.52904       [10, 12000/20034] loss: 0.004
3684.49140       [10, 14000/20034] loss: 0.007
3700.41578       [10, 16000/20034] loss: 0.003
3716.30058       [10, 18000/20034] loss: 0.003
3732.27549       [10, 20000/20034] loss: 0.003
3733.61250       Epoch [10/10], Loss: 0.00078
3893.06089       Training SVM
3915.55377       [1,  2000/20034] loss: -9.852
3931.09738       [1,  4000/20034] loss: -29.245
3946.67541       [1,  6000/20034] loss: -48.632
3962.23874       [1,  8000/20034] loss: -67.994
3977.81105       [1, 10000/20034] loss: -87.369
3993.36816       [1, 12000/20034] loss: -106.722
4009.03778       [1, 14000/20034] loss: -126.154
4024.80665       [1, 16000/20034] loss: -145.566
4040.39475       [1, 18000/20034] loss: -164.897
4055.99580       [1, 20000/20034] loss: -184.242
4057.44560       Epoch [1/10], Loss: -193.92319
4079.92374       [2,  2000/20034] loss: -203.959
4095.55570       [2,  4000/20034] loss: -223.417
4111.17006       [2,  6000/20034] loss: -242.793
4126.84616       [2,  8000/20034] loss: -262.086
4142.52479       [2, 10000/20034] loss: -281.460
4158.14747       [2, 12000/20034] loss: -300.771
4173.83171       [2, 14000/20034] loss: -320.293
4189.44036       [2, 16000/20034] loss: -339.739
4205.07387       [2, 18000/20034] loss: -359.002
4220.75509       [2, 20000/20034] loss: -378.316
4222.11500       Epoch [2/10], Loss: -387.65274
4244.67617       [3,  2000/20034] loss: -398.048
4260.39445       [3,  4000/20034] loss: -417.576
4276.14088       [3,  6000/20034] loss: -436.942
4291.82209       [3,  8000/20034] loss: -456.167
4307.56003       [3, 10000/20034] loss: -475.544
4323.28509       [3, 12000/20034] loss: -494.814
4339.11102       [3, 14000/20034] loss: -514.431
4354.79936       [3, 16000/20034] loss: -533.919
4370.50433       [3, 18000/20034] loss: -553.121
4386.21400       [3, 20000/20034] loss: -572.409
4387.56031       Epoch [3/10], Loss: -581.40369
4410.21559       [4,  2000/20034] loss: -592.163
4426.03441       [4,  4000/20034] loss: -611.768
4441.81601       [4,  6000/20034] loss: -631.133
4457.70778       [4,  8000/20034] loss: -650.297
4473.47304       [4, 10000/20034] loss: -669.684
4489.26374       [4, 12000/20034] loss: -688.921
4505.13657       [4, 14000/20034] loss: -708.634
4520.94766       [4, 16000/20034] loss: -728.158
4536.71694       [4, 18000/20034] loss: -747.294
4552.52292       [4, 20000/20034] loss: -766.549
4554.06252       Epoch [4/10], Loss: -775.19897
4576.54271       [5,  2000/20034] loss: -786.319
4592.13493       [5,  4000/20034] loss: -805.994
4607.77989       [5,  6000/20034] loss: -825.350
4623.41742       [5,  8000/20034] loss: -844.447
4639.13052       [5, 10000/20034] loss: -863.836
4654.75209       [5, 12000/20034] loss: -883.033
4670.43592       [5, 14000/20034] loss: -902.839
4686.04833       [5, 16000/20034] loss: -922.399
4701.63716       [5, 18000/20034] loss: -941.466
4717.25283       [5, 20000/20034] loss: -960.690
4718.70136       Epoch [5/10], Loss: -968.99390
4741.33104       [6,  2000/20034] loss: -980.475
4756.98648       [6,  4000/20034] loss: -1000.220
4772.60805       [6,  6000/20034] loss: -1019.567
4788.23925       [6,  8000/20034] loss: -1038.604
4803.87926       [6, 10000/20034] loss: -1058.001
4819.48390       [6, 12000/20034] loss: -1077.163
4835.15331       [6, 14000/20034] loss: -1097.068
4850.79016       [6, 16000/20034] loss: -1116.669
4866.43905       [6, 18000/20034] loss: -1135.676
4882.10264       [6, 20000/20034] loss: -1154.874
4883.38834       Epoch [6/10], Loss: -1162.83508
4905.81042       [7,  2000/20034] loss: -1174.679
4921.42805       [7,  4000/20034] loss: -1194.497
4937.01758       [7,  6000/20034] loss: -1213.839
4952.62185       [7,  8000/20034] loss: -1232.809
4968.32236       [7, 10000/20034] loss: -1252.208
4983.91046       [7, 12000/20034] loss: -1271.330
4999.54645       [7, 14000/20034] loss: -1291.327
5015.17309       [7, 16000/20034] loss: -1310.964
5030.82996       [7, 18000/20034] loss: -1329.903
5046.43962       [7, 20000/20034] loss: -1349.069
5047.81672       Epoch [7/10], Loss: -1356.68372
5070.16382       [8,  2000/20034] loss: -1368.889
5085.73512       [8,  4000/20034] loss: -1388.777
5101.40825       [8,  6000/20034] loss: -1408.110
5116.93730       [8,  8000/20034] loss: -1427.013
5132.48746       [8, 10000/20034] loss: -1446.415
5148.03135       [8, 12000/20034] loss: -1465.497
5163.63615       [8, 14000/20034] loss: -1485.586
5179.17608       [8, 16000/20034] loss: -1505.259
5194.76225       [8, 18000/20034] loss: -1524.130
5210.42693       [8, 20000/20034] loss: -1543.264
5211.69276       Epoch [8/10], Loss: -1550.53320
5234.31249       [9,  2000/20034] loss: -1563.100
5249.94092       [9,  4000/20034] loss: -1583.058
5265.59943       [9,  6000/20034] loss: -1602.381
5281.22003       [9,  8000/20034] loss: -1621.217
5296.83135       [9, 10000/20034] loss: -1640.621
5312.47308       [9, 12000/20034] loss: -1659.663
5329.04073       [9, 14000/20034] loss: -1679.845
5345.48998       [9, 16000/20034] loss: -1699.553
5361.16164       [9, 18000/20034] loss: -1718.357
5376.83414       [9, 20000/20034] loss: -1737.460
5378.39681       Epoch [9/10], Loss: -1744.38281
5404.89645       [10,  2000/20034] loss: -1757.311
5420.41187       [10,  4000/20034] loss: -1777.338
5436.02075       [10,  6000/20034] loss: -1796.652
5451.60758       [10,  8000/20034] loss: -1815.421
5467.21360       [10, 10000/20034] loss: -1834.828
5482.80301       [10, 12000/20034] loss: -1853.830
5498.49339       [10, 14000/20034] loss: -1874.104
5514.08684       [10, 16000/20034] loss: -1893.848
5529.66085       [10, 18000/20034] loss: -1912.584
5545.20971       [10, 20000/20034] loss: -1931.654
5546.71005       Epoch [10/10], Loss: -1938.23230
5706.27244       Testing models
5706.27344       Testing LSTM
5764.91900       Testing ANN
5765.23217       Testing SVM
5765.29310       Calculating metrics
Accuracy_LSTM Train:    0.929835        Precision_LSTM Train:   0.500259
Recall_LSTM Train:      0.500253        F1_LSTM Train:          0.500253
AUC_LSTM Train:         0.500253
Matrix_LSTM Train:
[[   14628   386150]
 [  377106 10100068]]
Accuracy_ANN Train:     0.999238        Precision_ANN Train:    0.999352
Recall_ANN Train:       0.989909        F1_ANN Train:           0.994582
AUC_ANN Train:  0.989909
Matrix_ANN Train:
[[  392697     8081]
 [     206 10476968]]
c:\Users\rodyj\miniconda3\envs\python3.11\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Accuracy_SVM Train:     0.036843        Precision_SVM Train:    0.018422
Recall_SVM Train:       0.500000        F1_SVM Train:           0.035534
AUC_SVM Train:  0.500000
Matrix_SVM Train:
[[  400778        0]
 [10477174        0]]
Accuracy_LSTM Test:     0.999100        Precision_LSTM Test:    0.999213
Recall_LSTM Test:       0.988085        F1_LSTM Test:           0.993581
AUC_LSTM Test:  0.988085
Matrix_LSTM Test:
[[  97682    2382]
 [     65 2619359]]
Accuracy_ANN Test:      0.999196        Precision_ANN Test:     0.999239
Recall_ANN Test:        0.989413        F1_ANN Test:            0.994274
AUC_ANN Test:   0.989413
Matrix_ANN Test:
[[  97948    2116]
 [     70 2619354]]
Accuracy_SVM Test:      0.019765        Precision_SVM Test:     0.010075
Recall_SVM Test:        0.268573        F1_SVM Test:            0.019382
AUC_SVM Test:   0.268573
Matrix_SVM Test:
[[  53749   46315]
 [2619422       2]]
5814.20082       Loading encoded dataset from .\encoded_dataset2.pkl
5814.40025       Testing LSTM
5851.46879       Testing ANN
5851.60779       Testing SVM
5851.61482       Calculating validation metrics
Accuracy_LSTM Validate: 0.826179        Precision_LSTM Validate:        0.568562
Recall_LSTM Validate:   0.889316        F1_LSTM Validate:               0.572054
AUC_LSTM Validate:      0.889316
Matrix_LSTM Validate:
[[  4338    198]
 [ 26936 124631]]
Accuracy_ANN Validate:  0.797429        Precision_ANN Validate:         0.522917
Recall_ANN Validate:    0.629854        F1_ANN Validate:                0.500202
AUC_ANN Validate:       0.629854
Matrix_ANN Validate:
[[  2050   2486]
 [ 29136 122431]]
Accuracy_SVM Validate:  0.029045        Precision_SVM Validate:         0.139520
Recall_SVM Validate:    0.499673        F1_SVM Validate:                0.028226
AUC_SVM Validate:       0.499673
Matrix_SVM Validate:
[[  4533      3]
 [151566      1]]
5852.33153       Training Meta-Learner
5852.33253       Training MetaLearner
5881.94094       Epoch [10/10], Loss: 0.1447619
5882.04895       Testing MetaLeaner
5882.05598       Testing MetaLeaner
5882.10079       Calculating metrics for Meta-Learner
c:\Users\rodyj\miniconda3\envs\python3.11\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Accuracy_Meta-Learner Train:    0.963157        Precision_Meta-Learner Train:   0.481578
Recall_Meta-Learner Train:      0.500000        F1_Meta-Learner Train:          0.490616
AUC_Meta-Learner Train:         0.500000
Matrix_Meta-Learner Train:
[[       0   400778]
 [       0 10477174]]
c:\Users\rodyj\miniconda3\envs\python3.11\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Accuracy_Meta-Learner Test:     0.963205        Precision_Meta-Learner Test:    0.481602
Recall_Meta-Learner Test:       0.500000        F1_Meta-Learner Test:           0.490629
AUC_Meta-Learner Test:  0.500000
Matrix_Meta-Learner Test:
[[      0  100064]
 [      0 2619424]]
5898.56525       Evaluating MetaLeaner
5898.64125       Calculating validation metrics for Meta-Learner
c:\Users\rodyj\miniconda3\envs\python3.11\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
Accuracy_Meta-Learner Validate: 0.970942        Precision_Meta-Learner Validate:        0.485471
Recall_Meta-Learner Validate:   0.500000        F1_Meta-Learner Validate:               0.492628
AUC_Meta-Learner Validate:      0.500000
Matrix_Meta-Learner Validate:
[[     0   4536]
 [     0 151567]]
